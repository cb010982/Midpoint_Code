# -*- coding: utf-8 -*-
"""Copy of FinalDiabetesPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a5gI1xi6u6lj-GiH_KHfoYGIHDu0u9Ok

# **Imports**
"""

#Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.subplots as sp
import plotly.graph_objects as go
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from imblearn.combine import SMOTEENN
from collections import Counter
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
!pip install catboost
from catboost import CatBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score
import pandas as pd
from IPython.display import display, Markdown
import warnings
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.model_selection import GridSearchCV
import joblib
from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score
from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score


warnings.filterwarnings('ignore')

"""# **Analyse Dataset**"""

#Importing the dataset
from google.colab import files
uploaded = files.upload()

#Print head of dataset
df = pd.read_csv("PIMA_Dataset.csv")
print("First 5 rows of the dataset:")
print(df.head())

#Column names
print("\nColumn names:", df.columns)

#Shape of dataset
print("\nShape of the dataset (rows, columns):", df.shape)

#Dataset Information
print("\nDataset Information:")
print(df.info())

#Summary Staistics
print("\nSummary Statistics:")
print(df.describe())

#Missing values
print("\nMissing values in the dataset:")
print(df.isnull().sum())

#Duplicate Values
print("\nNumber of duplicate rows:", df.duplicated().sum())

# Replace zeros with NaN for relevant features
zero_columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[zero_columns] = df[zero_columns].replace(0, np.nan)
# Check for NaNs and impute (median)
df.fillna(df.median(), inplace=True)

"""# **Exploratory Data Analysis**

### **Class Distribution**
"""

#Class Distribution
plt.style.use("dark_background")
plt.figure(figsize=(6, 4))
sns.countplot(x=df.iloc[:, -1])
plt.title("Class Distribution")
plt.xlabel("Target Variable")
plt.ylabel("Count")
plt.show()

"""### **Glucose Distribution**"""

# Glucose EDA
plt.style.use("dark_background")
feature = "Glucose"
target_col = df.columns[-1]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

#  Histogram
sns.histplot(df[feature], bins=30, kde=True, ax=axes[0], color="cyan", edgecolor="black")
axes[0].set_title(f"Histogram of {feature}", fontsize=14, color="white")

# Boxplot
sns.boxplot(x=df[feature], ax=axes[1], color="orange")
axes[1].set_title(f"Boxplot of {feature}", fontsize=14, color="white")

# KDE Plot (by Outcome)
sns.kdeplot(df[df[target_col] == 0][feature], label="No Diabetes", fill=True, color="lime", ax=axes[2])
sns.kdeplot(df[df[target_col] == 1][feature], label="Diabetes", fill=True, color="red", ax=axes[2])
axes[2].set_title(f"KDE Plot of {feature} by {target_col}", fontsize=14, color="white")
axes[2].legend()

for ax in axes:
    ax.grid(color="gray", linestyle="dashed", linewidth=0.5)
    ax.spines["top"].set_color("white")
    ax.spines["bottom"].set_color("white")
    ax.spines["left"].set_color("white")
    ax.spines["right"].set_color("white")

plt.suptitle(f"Exploratory Analysis of {feature}", fontsize=16, color="white")
plt.tight_layout()
plt.show()

"""### **Blood Pressure Distribution**"""

# Blood Pressure EDA
plt.style.use("dark_background")
feature = "BloodPressure"
target_col = df.columns[-1]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Histogram
sns.histplot(df[feature], bins=30, kde=True, ax=axes[0], color="cyan", edgecolor="black")
axes[0].set_title(f"Histogram of {feature}", fontsize=14, color="white")

# Boxplot
sns.boxplot(x=df[feature], ax=axes[1], color="orange")
axes[1].set_title(f"Boxplot of {feature}", fontsize=14, color="white")

# KDE Plot (by Outcome)
sns.kdeplot(df[df[target_col] == 0][feature], label="No Diabetes", fill=True, color="lime", ax=axes[2])
sns.kdeplot(df[df[target_col] == 1][feature], label="Diabetes", fill=True, color="red", ax=axes[2])
axes[2].set_title(f"KDE Plot of {feature} by {target_col}", fontsize=14, color="white")
axes[2].legend()


for ax in axes:
    ax.grid(color="gray", linestyle="dashed", linewidth=0.5)
    ax.spines["top"].set_color("white")
    ax.spines["bottom"].set_color("white")
    ax.spines["left"].set_color("white")
    ax.spines["right"].set_color("white")

plt.suptitle(f"Exploratory Analysis of {feature}", fontsize=16, color="white")
plt.tight_layout()
plt.show()

"""### **Insulin Distribution**"""

# Insulin EDA

plt.style.use("dark_background")
feature = "Insulin"
target_col = df.columns[-1]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

#  Histogram
sns.histplot(df[feature], bins=30, kde=True, ax=axes[0], color="cyan", edgecolor="black")
axes[0].set_title(f"Histogram of {feature}", fontsize=14, color="white")

# Boxplot
sns.boxplot(x=df[feature], ax=axes[1], color="orange")
axes[1].set_title(f"Boxplot of {feature}", fontsize=14, color="white")

# KDE Plot (by Outcome)
sns.kdeplot(df[df[target_col] == 0][feature], label="No Diabetes", fill=True, color="lime", ax=axes[2])
sns.kdeplot(df[df[target_col] == 1][feature], label="Diabetes", fill=True, color="red", ax=axes[2])
axes[2].set_title(f"KDE Plot of {feature} by {target_col}", fontsize=14, color="white")
axes[2].legend()

for ax in axes:
    ax.grid(color="gray", linestyle="dashed", linewidth=0.5)
    ax.spines["top"].set_color("white")
    ax.spines["bottom"].set_color("white")
    ax.spines["left"].set_color("white")
    ax.spines["right"].set_color("white")

plt.suptitle(f"Exploratory Analysis of {feature}", fontsize=16, color="white")
plt.tight_layout()
plt.show()

"""### **BMI Distribution**"""

# BMI EDA

plt.style.use("dark_background")
feature = "BMI"
target_col = df.columns[-1]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Histogram
sns.histplot(df[feature], bins=30, kde=True, ax=axes[0], color="cyan", edgecolor="black")
axes[0].set_title(f"Histogram of {feature}", fontsize=14, color="white")

#  Boxplot
sns.boxplot(x=df[feature], ax=axes[1], color="orange")
axes[1].set_title(f"Boxplot of {feature}", fontsize=14, color="white")

# KDE Plot (by Outcome)
sns.kdeplot(df[df[target_col] == 0][feature], label="No Diabetes", fill=True, color="lime", ax=axes[2])
sns.kdeplot(df[df[target_col] == 1][feature], label="Diabetes", fill=True, color="red", ax=axes[2])
axes[2].set_title(f"KDE Plot of {feature} by {target_col}", fontsize=14, color="white")
axes[2].legend()

for ax in axes:
    ax.grid(color="gray", linestyle="dashed", linewidth=0.5)
    ax.spines["top"].set_color("white")
    ax.spines["bottom"].set_color("white")
    ax.spines["left"].set_color("white")
    ax.spines["right"].set_color("white")

plt.suptitle(f"Exploratory Analysis of {feature}", fontsize=16, color="white")
plt.tight_layout()
plt.show()

"""### **Age Distribution**"""

# Age EDA

plt.style.use("dark_background")
feature = "Age"
target_col = df.columns[-1]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Histogram
sns.histplot(df[feature], bins=30, kde=True, ax=axes[0], color="cyan", edgecolor="black")
axes[0].set_title(f"Histogram of {feature}", fontsize=14, color="white")

# Boxplot
sns.boxplot(x=df[feature], ax=axes[1], color="orange")
axes[1].set_title(f"Boxplot of {feature}", fontsize=14, color="white")

# KDE Plot (by Outcome)
sns.kdeplot(df[df[target_col] == 0][feature], label="No Diabetes", fill=True, color="lime", ax=axes[2])
sns.kdeplot(df[df[target_col] == 1][feature], label="Diabetes", fill=True, color="red", ax=axes[2])
axes[2].set_title(f"KDE Plot of {feature} by {target_col}", fontsize=14, color="white")
axes[2].legend()

for ax in axes:
    ax.grid(color="gray", linestyle="dashed", linewidth=0.5)
    ax.spines["top"].set_color("white")
    ax.spines["bottom"].set_color("white")
    ax.spines["left"].set_color("white")
    ax.spines["right"].set_color("white")

plt.suptitle(f"Exploratory Analysis of {feature}", fontsize=16, color="white")
plt.tight_layout()
plt.show()

"""### **Skin Thickness Distribution**"""

#Skin Thickness EDA

plt.style.use("dark_background")
feature = "SkinThickness"
target_col = df.columns[-1]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Histogram
sns.histplot(df[feature], bins=30, kde=True, ax=axes[0], color="cyan", edgecolor="black")
axes[0].set_title(f"Histogram of {feature}", fontsize=14, color="white")

# Boxplot
sns.boxplot(x=df[feature], ax=axes[1], color="orange")
axes[1].set_title(f"Boxplot of {feature}", fontsize=14, color="white")

# KDE Plot (by Outcome)
sns.kdeplot(df[df[target_col] == 0][feature], label="No Diabetes", fill=True, color="lime", ax=axes[2])
sns.kdeplot(df[df[target_col] == 1][feature], label="Diabetes", fill=True, color="red", ax=axes[2])
axes[2].set_title(f"KDE Plot of {feature} by {target_col}", fontsize=14, color="white")
axes[2].legend()

for ax in axes:
    ax.grid(color="gray", linestyle="dashed", linewidth=0.5)
    ax.spines["top"].set_color("white")
    ax.spines["bottom"].set_color("white")
    ax.spines["left"].set_color("white")
    ax.spines["right"].set_color("white")

plt.suptitle(f"Exploratory Analysis of {feature}", fontsize=16, color="white")
plt.tight_layout()
plt.show()

"""### **Diabetes Pedigree Function Distribution**"""

#Diabetes Pedigree Function EDA

plt.style.use("dark_background")
feature = "DiabetesPedigreeFunction"
target_col = df.columns[-1]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

#  Histogram
sns.histplot(df[feature], bins=30, kde=True, ax=axes[0], color="cyan", edgecolor="black")
axes[0].set_title(f"Histogram of {feature}", fontsize=14, color="white")

#  Boxplot
sns.boxplot(x=df[feature], ax=axes[1], color="orange")
axes[1].set_title(f"Boxplot of {feature}", fontsize=14, color="white")

#  KDE Plot (by Outcome)
sns.kdeplot(df[df[target_col] == 0][feature], label="No Diabetes", fill=True, color="lime", ax=axes[2])
sns.kdeplot(df[df[target_col] == 1][feature], label="Diabetes", fill=True, color="red", ax=axes[2])
axes[2].set_title(f"KDE Plot of {feature} by {target_col}", fontsize=14, color="white")
axes[2].legend()


for ax in axes:
    ax.grid(color="gray", linestyle="dashed", linewidth=0.5)
    ax.spines["top"].set_color("white")
    ax.spines["bottom"].set_color("white")
    ax.spines["left"].set_color("white")
    ax.spines["right"].set_color("white")

plt.suptitle(f"Exploratory Analysis of {feature}", fontsize=16, color="white")
plt.tight_layout()
plt.show()

"""### **Pregnancy Distribution**"""

# Pregnancy EDA
plt.style.use("dark_background")
feature = "Pregnancies"
target_col = df.columns[-1]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Histogram
sns.histplot(df[feature], bins=30, kde=True, ax=axes[0], color="cyan", edgecolor="black")
axes[0].set_title(f"Histogram of {feature}", fontsize=14, color="white")

# Boxplot
sns.boxplot(x=df[feature], ax=axes[1], color="orange")
axes[1].set_title(f"Boxplot of {feature}", fontsize=14, color="white")

# KDE Plot (by Outcome)
sns.kdeplot(df[df[target_col] == 0][feature], label="No Diabetes", fill=True, color="lime", ax=axes[2])
sns.kdeplot(df[df[target_col] == 1][feature], label="Diabetes", fill=True, color="red", ax=axes[2])
axes[2].set_title(f"KDE Plot of {feature} by {target_col}", fontsize=14, color="white")
axes[2].legend()

for ax in axes:
    ax.grid(color="gray", linestyle="dashed", linewidth=0.5)
    ax.spines["top"].set_color("white")
    ax.spines["bottom"].set_color("white")
    ax.spines["left"].set_color("white")
    ax.spines["right"].set_color("white")

plt.suptitle(f"Exploratory Analysis of {feature}", fontsize=16, color="white")
plt.tight_layout()
plt.show()

# HeatMap
corr_matrix = df.corr()
plt.style.use("dark_background")
plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", linewidths=0.5, fmt=".2f")
plt.title("Feature Correlation Heatmap", fontsize=14, color="white")
plt.show()

"""# **Preprocessing**"""

# Splitting features and target
features = df.drop(columns=['Outcome'])
target = df['Outcome']

# Outlier Handling using IQR method
for feature in features.columns:
    q1_value = features[feature].quantile(0.25)
    q3_value = features[feature].quantile(0.75)
    iqr_range = q3_value - q1_value
    lower_cutoff = q1_value - 1.5 * iqr_range
    upper_cutoff = q3_value + 1.5 * iqr_range
    features[feature] = np.clip(features[feature], lower_cutoff, upper_cutoff)


def display_boxplots(dataset, heading):
    plot_figure = sp.make_subplots(rows=2, cols=4, subplot_titles=features.columns)
    colors = ["cyan", "magenta", "yellow", "lime", "orange", "red", "purple", "blue"]

    row_idx, col_idx = 1, 1
    for index, variable in enumerate(features.columns):
        plot_figure.add_trace(go.Box(y=dataset[variable], name=variable, marker=dict(color=colors[index % len(colors)])), row=row_idx, col=col_idx)
        col_idx += 1
        if col_idx > 4:
            row_idx += 1
            col_idx = 1

    plot_figure.update_layout(
        title_text=heading,
        showlegend=False,
        height=800,
        width=1000,
        template="plotly_dark",
        font=dict(color="white"),
        paper_bgcolor="black",
        plot_bgcolor="black"
    )
    plot_figure.show()

display_boxplots(df, "Box Plots Before Outlier Capping")

display_boxplots(features, "Box Plots After Outlier Capping")

# Applying MinMax Scaling
scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(features)
scaled_features = pd.DataFrame(scaled_features, columns=features.columns)

# Merging the scaled features with the target variable
final_dataset = pd.concat([scaled_features, target.reset_index(drop=True)], axis=1)

print("First 5 rows of the processed dataset:")
print(final_dataset.head())

print("\nMinimum values after scaling:\n", final_dataset.min())
print("\nMaximum values after scaling:\n", final_dataset.max())

"""# Train Test Split"""

#Split Test and Train
X = final_dataset.drop(columns=['Outcome'])
y = final_dataset['Outcome']

# Split into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training Set: {X_train.shape[0]} samples")
print(f"Testing Set: {X_test.shape[0]} samples")

#smote to balance dataset
smote = SMOTE(random_state=42)
X_resampledfinal, y_resampledfinal = smote.fit_resample(features, target)

print("Class distribution before SMOTE:", Counter(target))
print("Class distribution after SMOTE:", Counter(y_resampledfinal))

# Split the resampled dataset into training (80%) and testing (20%) sets
X_train_resampledfinal, X_test_resampledfinal, y_train_resampledfinal, y_test_resampledfinal = train_test_split(
    X_resampledfinal, y_resampledfinal, test_size=0.2, random_state=42, stratify=y_resampledfinal
)
print(f"Training Set: {X_train_resampledfinal.shape[0]} samples")
print(f"Testing Set: {X_test_resampledfinal.shape[0]} samples")

"""# **Data Modeling**

The following models will be initially trained:

| Model | Type | Strength |
|--------|-----------------|----------------|
| **Logistic Regression** | Baseline | Simple & interpretable |
| **Decision Tree** | Tree-based | Feature importance & explainability |
| **Random Forest** | Ensemble | High accuracy & less overfitting |
| **XGBoost**  | Boosting | Best for high accuracy |
| **LightGBM**  | Boosting | Faster than XGBoost |
| **CatBoost**  | Boosting | Handles categorical data well |
| **KNN**  | Distance-based | Good for smaller datasets |
| **MLP Neural Network** | Deep Learning | Captures complex patterns |
| **Naïve Bayes**  | Probabilistic | Fast and efficient |
"""

#Initialising the models
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss"),
    "CatBoost": CatBoostClassifier(verbose=0, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "MLP Neural Network": MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42),
    "Naïve Bayes": GaussianNB(),
    "LGBM": LGBMClassifier(verbose=-1)

}

"""## NO SMOTE

*Doing test and train split on original dataset (no smote)*
"""

# Train and evaluate models
results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Compute evaluation metrics
    acc = accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, output_dict=True)

    # Store results
    results.append([
        name, acc, roc_auc, report["1"]["precision"], report["1"]["recall"], report["1"]["f1-score"]
    ])

# Convert results to DataFrame
df_results = pd.DataFrame(results, columns=["Model", "Accuracy", "ROC-AUC", "Precision", "Recall", "F1-Score"])
table_md = "### Model Performance Comparison (Without SMOTE)\n\n"
table_md += "| Model | Accuracy | ROC-AUC | Precision | Recall | F1-Score |\n"
table_md += "|-------|----------|---------|-----------|--------|----------|\n"
for row in results:
    table_md += f"| {row[0]} | {row[1]:.4f} | {row[2]:.4f} | {row[3]:.4f} | {row[4]:.4f} | {row[5]:.4f} |\n"

display(Markdown(table_md))

"""## WITH SMOTEENN

*Doing test and train split on smoteenn data*
"""

# Train and evaluate models
results = []
for name, model in models.items():
    model.fit(X_train_resampledfinal, y_train_resampledfinal)
    y_pred = model.predict(X_test_resampledfinal)

    acc = accuracy_score(y_test_resampledfinal, y_pred)
    roc_auc = roc_auc_score(y_test_resampledfinal, y_pred)
    report = classification_report(y_test_resampledfinal, y_pred, output_dict=True)

    # Store results
    results.append([
        name, acc, roc_auc, report["1"]["precision"], report["1"]["recall"], report["1"]["f1-score"]
    ])

# Convert results to DataFrame
df_results = pd.DataFrame(results, columns=["Model", "Accuracy", "ROC-AUC", "Precision", "Recall", "F1-Score"])
table_md = "### Model Performance Comparison (With SMOTE)\n\n"
table_md += "| Model | Accuracy | ROC-AUC | Precision | Recall | F1-Score |\n"
table_md += "|-------|----------|---------|-----------|--------|----------|\n"
for row in results:
    table_md += f"| {row[0]} | {row[1]:.4f} | {row[2]:.4f} | {row[3]:.4f} | {row[4]:.4f} | {row[5]:.4f} |\n"

display(Markdown(table_md))

"""# Cross Validation

## NO SMOTEENN

*Doing cross validation on original dataset (no smote)*
"""

# Cross-validation on original dataset
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
results_cv = []

for name, model in models.items():
    acc_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
    roc_auc_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')
    prec_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='precision')
    recall_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='recall')
    f1_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')

    results_cv.append([
        name, acc_scores.mean(), roc_auc_scores.mean(),
        prec_scores.mean(), recall_scores.mean(), f1_scores.mean()
    ])

# Convert results to DataFrame
df_results_cv = pd.DataFrame(results_cv, columns=["Model", "Accuracy", "ROC-AUC", "Precision", "Recall", "F1-Score"])

# Select the best model based on highest Accuracy
best_model_name = df_results_cv.sort_values("Accuracy", ascending=False).iloc[0]["Model"]
best_model = models[best_model_name]

# Train the best model on full training data
best_model.fit(X_train, y_train)

# Evaluate on test set
y_pred = best_model.predict(X_test)

test_results = {
    "Accuracy": accuracy_score(y_test, y_pred),
    "ROC-AUC": roc_auc_score(y_test, y_pred),
    "Precision": precision_score(y_test, y_pred),
    "Recall": recall_score(y_test, y_pred),
    "F1-Score": f1_score(y_test, y_pred)
}

# Display Cross-Validation Results
table_md = "### Model Performance Comparison (Cross-Validation - No SMOTE)\n\n"
table_md += "| Model | Accuracy | ROC-AUC | Precision | Recall | F1-Score |\n"
table_md += "|-------|----------|---------|-----------|--------|----------|\n"
for row in results_cv:
    table_md += f"| {row[0]} | {row[1]:.4f} | {row[2]:.4f} | {row[3]:.4f} | {row[4]:.4f} | {row[5]:.4f} |\n"


display(Markdown(table_md))

"""## WITH SMOTE"""

# Cross-validation strategy on SMOTE dataset
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
results_cv_smote = []

for name, model in models.items():
    acc_scores = cross_val_score(model, X_train_resampledfinal, y_train_resampledfinal, cv=cv, scoring='accuracy', n_jobs=-1)
    roc_auc_scores = cross_val_score(model, X_train_resampledfinal, y_train_resampledfinal, cv=cv, scoring='roc_auc', n_jobs=-1)
    prec_scores = cross_val_score(model, X_train_resampledfinal, y_train_resampledfinal, cv=cv, scoring='precision', n_jobs=-1)
    recall_scores = cross_val_score(model, X_train_resampledfinal, y_train_resampledfinal, cv=cv, scoring='recall', n_jobs=-1)
    f1_scores = cross_val_score(model, X_train_resampledfinal, y_train_resampledfinal, cv=cv, scoring='f1', n_jobs=-1)

    # Store mean results
    results_cv_smote.append([
        name, acc_scores.mean(), roc_auc_scores.mean(),
        prec_scores.mean(), recall_scores.mean(), f1_scores.mean()
    ])

df_results_cv_smote = pd.DataFrame(results_cv_smote, columns=["Model", "Accuracy", "ROC-AUC", "Precision", "Recall", "F1-Score"])

# Select the best model based on highest Accuracy
best_model_name_smote = df_results_cv_smote.sort_values("Accuracy", ascending=False).iloc[0]["Model"]
best_model_smote = models[best_model_name_smote]

# Train the best model on full SMOTE training data
best_model_smote.fit(X_train_resampledfinal, y_train_resampledfinal)

# Evaluate on  test set
y_pred_smote = best_model_smote.predict(X_test_resampledfinal)

test_results_smote = {
    "Accuracy": accuracy_score(y_test_resampledfinal, y_pred_smote),
    "ROC-AUC": roc_auc_score(y_test_resampledfinal, y_pred_smote),
    "Precision": precision_score(y_test_resampledfinal, y_pred_smote),
    "Recall": recall_score(y_test_resampledfinal, y_pred_smote),
    "F1-Score": f1_score(y_test_resampledfinal, y_pred_smote)
}

# Display Cross-Validation Results
table_md_smote = "### Model Performance Comparison (Cross-Validation on SMOTE Dataset)\n\n"
table_md_smote += "| Model | Accuracy | ROC-AUC | Precision | Recall | F1-Score |\n"
table_md_smote += "|-------|----------|---------|-----------|--------|----------|\n"
for row in results_cv_smote:
    table_md_smote += f"| {row[0]} | {row[1]:.4f} | {row[2]:.4f} | {row[3]:.4f} | {row[4]:.4f} | {row[5]:.4f} |\n"


display(Markdown(table_md_smote))

"""# Hyperparameter Tuning Grid Search"""

# Define hyperparameter grids
param_grids = {
    "Logistic Regression": {
        "C": [0.01, 0.1, 1, 10, 100],
        "solver": ["liblinear", "lbfgs"]
    },
    "Decision Tree": {
        "max_depth": [3, 5, 10, None],
        "criterion": ["gini", "entropy"]
    },
    "Random Forest": {
        "n_estimators": [50, 100, 200],
        "max_depth": [None, 10, 20]
    },
    "XGBoost": {
        "learning_rate": [0.01, 0.1, 0.2],
        "max_depth": [3, 5, 7]
    },
    "CatBoost": {
        "depth": [4, 6, 10],
        "learning_rate": [0.01, 0.1, 0.2]
    },
    "KNN": {
        "n_neighbors": [3, 5, 7, 10],
        "weights": ["uniform", "distance"]
    },
    "MLP Neural Network": {
        "hidden_layer_sizes": [(32,), (64, 32), (128, 64, 32)],
        "activation": ["relu", "tanh"],
        "learning_rate_init": [0.001, 0.01]
    },
    "Naïve Bayes": {},
    "LGBM": {
        "learning_rate": [0.01, 0.1, 0.2],
        "num_leaves": [20, 31, 40]
    }
}

"""## NO SMOTEENN"""

best_models = {}
best_params = {}

# Perform Grid Search
for name, model in models.items():
    print(f"Running Grid Search for {name}...")

    if name in param_grids and param_grids[name]:
        grid_search = GridSearchCV(model, param_grids[name], cv=cv, scoring="accuracy", n_jobs=-1)
        grid_search.fit(X_train, y_train)

        best_models[name] = grid_search.best_estimator_
        best_params[name] = grid_search.best_params_
        print(f"Best params for {name}: {grid_search.best_params_}")
    else:
        best_models[name] = model
        best_params[name] = "Default"

print("\nGrid Search Complete. Best models are stored.")

best_results = []

# Evaluate each best model
for name, model in best_models.items():
    print(f"Evaluating Best Model for {name}...")

    acc_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
    roc_auc_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')
    prec_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='precision')
    recall_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='recall')
    f1_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')

    # Train the best model on full training data
    model.fit(X_train, y_train)

    # Evaluate on test set
    y_pred = model.predict(X_test)

    test_results = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "ROC-AUC": roc_auc_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1-Score": f1_score(y_test, y_pred)
    }

    # Store results
    best_results.append([
        name,
        acc_scores.mean(), roc_auc_scores.mean(),
        prec_scores.mean(), recall_scores.mean(), f1_scores.mean(),
        best_params[name],
        test_results["Accuracy"], test_results["ROC-AUC"], test_results["Precision"],
        test_results["Recall"], test_results["F1-Score"]
    ])

df_best_results = pd.DataFrame(
    best_results,
    columns=[
        "Model", "CV Accuracy", "CV ROC-AUC", "CV Precision", "CV Recall", "CV F1-Score",
        "Best Hyperparameters", "Test Accuracy", "Test ROC-AUC", "Test Precision", "Test Recall", "Test F1-Score"
    ]
)

table_md_best = "### Best Models Performance (After Grid Search & Final Test Evaluation)\n\n"
table_md_best += "| Model | CV Accuracy | CV ROC-AUC | CV Precision | CV Recall | CV F1-Score | Best Hyperparameters | Test Accuracy | Test ROC-AUC | Test Precision | Test Recall | Test F1-Score |\n"
table_md_best += "|-------|------------|------------|-------------|-----------|------------|----------------------|--------------|------------|--------------|-----------|------------|\n"

for row in best_results:
    table_md_best += f"| {row[0]} | {row[1]:.4f} | {row[2]:.4f} | {row[3]:.4f} | {row[4]:.4f} | {row[5]:.4f} | {row[6]} | {row[7]:.4f} | {row[8]:.4f} | {row[9]:.4f} | {row[10]:.4f} | {row[11]:.4f} |\n"

display(Markdown(table_md_best))

"""## WITH SMOTE"""

# Store best models and parameters
best_models_smote = {}
best_params_smote = {}

# Perform Grid Search on SMOTEd Data
for name, model in models.items():
    print(f"Running Grid Search for {name} on SMOTEd dataset...")

    if name in param_grids and param_grids[name]:
        grid_search = GridSearchCV(model, param_grids[name], cv=cv, scoring="accuracy", n_jobs=-1)
        grid_search.fit(X_train_resampledfinal, y_train_resampledfinal)

        best_models_smote[name] = grid_search.best_estimator_
        best_params_smote[name] = grid_search.best_params_
        print(f"Best params for {name}: {grid_search.best_params_}")
    else:
        best_models_smote[name] = model
        best_params_smote[name] = "Default"

print("\nGrid Search Complete on SMOTEd dataset. Best models are stored.")

# store best results after Grid Search on SMOTEd data
best_results_smote = []

for name, model in best_models_smote.items():
    print(f"Evaluating Best Model for {name} on SMOTEd dataset...")

    # Perform Cross-Validation
    acc_scores = cross_val_score(model, X_train_resampledfinal, y_train_resampledfinal, cv=cv, scoring='accuracy')
    cv_accuracy = acc_scores.mean()

    #  best model on the SMOTEd training data
    model.fit(X_train_resampledfinal, y_train_resampledfinal)
    y_pred_smote = model.predict(X_test_resampledfinal)

    acc = accuracy_score(y_test_resampledfinal, y_pred_smote)
    roc_auc = roc_auc_score(y_test_resampledfinal, y_pred_smote)
    report = classification_report(y_test_resampledfinal, y_pred_smote, output_dict=True)

    best_results_smote.append([
        name, cv_accuracy, acc, roc_auc, report["1"]["precision"], report["1"]["recall"], report["1"]["f1-score"],
        best_params_smote[name]
    ])

# Convert to DataFrame
df_best_results_smote = pd.DataFrame(
    best_results_smote,
    columns=["Model", "CV Accuracy", "Test Accuracy", "ROC-AUC", "Precision", "Recall", "F1-Score", "Best Hyperparameters"]
)

table_md_best_smote = "### Best Models Performance (After Grid Search on SMOTEd Dataset)\n\n"
table_md_best_smote += "| Model | CV Accuracy | Test Accuracy | ROC-AUC | Precision | Recall | F1-Score | Best Hyperparameters |\n"
table_md_best_smote += "|-------|------------|---------------|---------|-----------|--------|----------|----------------------|\n"

for row in best_results_smote:
    table_md_best_smote += f"| {row[0]} | {row[1]:.4f} | {row[2]:.4f} | {row[3]:.4f} | {row[4]:.4f} | {row[5]:.4f} | {row[6]:.4f} | {row[7]} |\n"

display(Markdown(table_md_best_smote))

random_forest_model = best_models_smote['Random Forest']

# Save the model as a pickle file
joblib.dump(random_forest_model, 'diabetes_random_forest_model.pkl')

print("Random Forest model saved as 'diabetes_random_forest_model.pkl'")

# Commented out IPython magic to ensure Python compatibility.
from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve
import matplotlib.pyplot as plt

# Ensure inline plotting in Colab
# %matplotlib inline

# Retrieve the best Random Forest model
random_forest_model = best_models_smote['Random Forest']

# Make predictions
y_prob = random_forest_model.predict_proba(X_test_resampledfinal)[:, 1]  # Get probability for class 1
y_pred = random_forest_model.predict(X_test_resampledfinal)

# Compute AUC-ROC Score
auc_roc = roc_auc_score(y_test_resampledfinal, y_prob)

# Compute Confusion Matrix
cm = confusion_matrix(y_test_resampledfinal, y_pred)

# Display Confusion Matrix
plt.figure(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Diabetes", "Diabetes"])
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix - Random Forest")
plt.show()

# Plot ROC Curve with better visualization
fpr, tpr, _ = roc_curve(y_test_resampledfinal, y_prob)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color="blue", lw=2, label=f"AUC-ROC = {auc_roc:.4f}")
plt.plot([0, 1], [0, 1], color="gray", linestyle="--", lw=1)  # Random guessing line
plt.fill_between(fpr, tpr, alpha=0.2, color="blue")  # Shaded AUC area

# Labels & Formatting
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend(loc="lower right")
plt.grid(True, linestyle="--", alpha=0.7)

# Show the ROC Curve
plt.show()

# Print AUC-ROC Score
print(f"AUC-ROC Score: {auc_roc:.4f}")

from sklearn.metrics import classification_report

# Get the classification report
report = classification_report(y_test_resampledfinal, y_pred_smote, target_names=["No Diabetes", "Diabetes"])

# Print it
print("Classification Report:\n")
print(report)